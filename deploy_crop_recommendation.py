# -*- coding: utf-8 -*-
"""Deploy Crop Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HnPEs9FD6abNpRLx1Oi-pdeWuB4z_Oan

**DATASET INFO**
- This dataset was build by augmenting datasets of rainfall, climate and fertilizer data available for India.

**Data fields**
- N - ratio of Nitrogen content in soil
- P - ratio of Phosphorous content in soil
- K - ratio of Potassium content in soil
- temperature - temperature in degree Celsius
- humidity - relative humidity in %
- ph - ph value of the soil
- rainfall - rainfall in mm

# IMPORT LIBRARIES
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

data = pd.read_csv('Crop_recommendation.csv')
data

"""# DATA PREPROCESSING"""

data.info()

data.describe(include="all")

data.shape

data.isnull().sum()

data.apply(lambda x: len(x.unique()))

plt.figure(figsize = (8,6))
sns.heatmap(data.corr(), center = 0, annot = True, cmap="rocket")
plt.show()

data['label'].unique()

data['label'].value_counts()

data.columns

features = data[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]
target = data['label']
#labels = data['label']

for column in features.columns:
    plt.figure()             
    sns.distplot(data[column])

plt.figure(figsize=(7,7))
sns.pairplot(data, hue = "label")
plt.show()

"""# MACHINE LEARNING CLASSIFICATION MODELS"""

from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =2)

acc=[]  #stores accuracy
model=[] #stores model name

from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn import tree
import warnings
warnings.filterwarnings('ignore')

"""## 1. LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression

LogReg = LogisticRegression(multi_class='multinomial',solver='lbfgs')
LogReg.fit(Xtrain,Ytrain)
predicted_values = LogReg.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Logistic Regression')
print("Logistic Regression's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(LogReg,features,target,cv=5)
score

print('Training set score: {:.4f}'.format(LogReg.score(Xtrain, Ytrain)*100))
print('Test set score: {:.4f}'.format(LogReg.score(Xtest, Ytest)*100))

"""## 2. NAIVE BAYES"""

from sklearn.naive_bayes import GaussianNB

NaiveBayes = GaussianNB()

NaiveBayes.fit(Xtrain,Ytrain)

predicted_values = NaiveBayes.predict(Xtest)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Naive Bayes')
print("Naive Bayes's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(NaiveBayes,features,target,cv=5)
score

print('Training set score: {:.4f}'.format(NaiveBayes.score(Xtrain, Ytrain)))
print('Test set score: {:.4f}'.format(NaiveBayes.score(Xtest, Ytest)))

"""## 3. K-NEAREST NEIGHBOR"""

from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
scaler = StandardScaler()
scaler.fit(Xtrain)

X_train = scaler.transform(Xtrain)
X_test = scaler.transform(Xtest)

neighbors = np.arange(1, 10)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
  
# Loop over K values
for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, Ytrain)
      
    # Compute traning and test data accuracy
    train_accuracy[i] = knn.score(X_train, Ytrain)
    test_accuracy[i] = knn.score(X_test, Ytest)
  
# Generate plot
plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')
  
plt.legend()
plt.xlabel('n_neighbors')
plt.ylabel('Accuracy')
plt.show()

error = []
# Calculating error for K values between 1 and 40
for i in range(1, 10):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, Ytrain)
    pred_i = knn.predict(X_test)
    error.append(np.mean(pred_i != Ytest))

plt.figure(figsize=(8, 6))
plt.plot(range(1, 10), error, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')

#since MAX ACCURACY on test data & MINIMAL ERROR RATE at N=5 neighbours

KNN = KNeighborsClassifier(n_neighbors=5)
KNN.fit(X_train, Ytrain)

predicted_values = KNN.predict(X_test)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('KNN')
print("KNN's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(KNN,features,target,cv=5)
score

print('Training set score: {:.4f}'.format(KNN.score(Xtrain, Ytrain)))
print('Test set score: {:.4f}'.format(KNN.score(Xtest, Ytest)))

"""## 4. DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier

DecisionTree = DecisionTreeClassifier(criterion="entropy",max_depth=5)
DecisionTree.fit(Xtrain,Ytrain)

predicted_values = DecisionTree.predict(Xtest)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Decision Tree')
print("Decision Trees's Accuracy : ", x*100)
print()
print(classification_report(Ytest,predicted_values))

from sklearn.model_selection import cross_val_score
score = cross_val_score(DecisionTree, features, target,cv=5)
score

print('Training set score: {:.4f}'.format(DecisionTree.score(Xtrain, Ytrain)))
print('Test set score: {:.4f}'.format(DecisionTree.score(Xtest, Ytest)))

"""## 5. RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

RF = RandomForestClassifier()
RF.fit(Xtrain,Ytrain)

predicted_values = RF.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('RF')
print("RF's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(RF,features,target,cv=5)
score

print('Training set score: {:.4f}'.format(RF.score(Xtrain, Ytrain)))
print('Test set score: {:.4f}'.format(RF.score(Xtest, Ytest)))

"""## 6. XGBOOST"""

import xgboost as xgb
XB = xgb.XGBClassifier()
XB.fit(Xtrain,Ytrain)

predicted_values = XB.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('XGBoost')
print("XGBoost's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(XB,features,target,cv=5)
score

print('Training set score: {:.4f}'.format(XB.score(Xtrain, Ytrain)))
print('Test set score: {:.4f}'.format(XB.score(Xtest, Ytest)))

"""## 7. SUPPORT VECTOR MACHINE"""

from sklearn.svm import SVC

SVM = SVC()
SVM.fit(Xtrain,Ytrain)
predicted_values = SVM.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('SVM')
print("SVM's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))

score = cross_val_score(SVM,features,target,cv=5)
score

print('Training set score: {:.4f}'.format(SVM.score(Xtrain, Ytrain)))
print('Test set score: {:.4f}'.format(SVM.score(Xtest, Ytest)))

"""## ACCURACY COMPARISON OF MODELS"""

plt.figure(figsize=[14,6])
plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')
sns.barplot(x = acc,y = model,palette ='Spectral')

print("Accuracy of models \n")
accuracy_models = dict(zip(model, acc))
for k, v in accuracy_models.items():
    print (k, ' : ', v*100)

from tabulate import tabulate
df1 = pd.DataFrame(list(zip(model, acc)),
               columns =['Model','Accuracy'])
df1['Accuracy']=df1['Accuracy']*100
print(tabulate(df1, headers = ['Model','Accuracy'], tablefmt = 'github'))

"""## ACTUAL AND PREDICTED CROPS COMPARISON"""

predicted_values = RF.predict(Xtest)

df = pd.DataFrame(list(zip(Ytest,predicted_values)),
               columns =['Actual_Crops','Predicted_crops'])
print(df)

#values having error
print(df[df['Actual_Crops'] !=df['Predicted_crops']])

predicted_values = XB.predict(Xtest)

df = pd.DataFrame(list(zip(Ytest,predicted_values)),
               columns =['Actual_Crops','Predicted_crops'])
print(df)

#values having error
print(df[df['Actual_Crops'] !=df['Predicted_crops']])

data.columns

"""## CROPS RECOMMENDED FOR UNKNOWN VALUES USING HIGHEST ACCURACY MODELS"""

#Random Forest AND XGBoost is found to have the highest predict
dt = [[104,18, 30, 23.6, 60.3, 6.7, 140.91]]
dt = pd.DataFrame(dt,columns =['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'])
prediction1 = RF.predict(dt)
prediction2 = XB.predict(dt)
print("RF Predicted Crop : ",prediction1)
print("XB Predicted Crop : ",prediction2)

dt = np.array([[83, 45, 60, 28, 70.3, 7.0, 150.9]])
dt = pd.DataFrame(dt,columns =['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'])
prediction1 = RF.predict(dt)
prediction2 = XB.predict(dt)
print("RF Predicted Crop : ",prediction1)
print("XB Predicted Crop : ",prediction2)

dt = np.array([[40, 4, 10, 27.8, 100.56, 10.9, 200]])
dt = pd.DataFrame(dt,columns =['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'])
prediction1 = RF.predict(dt)
prediction2 = XB.predict(dt)
print("RF Predicted Crop : ",prediction1)
print("XB Predicted Crop : ",prediction2)

dt = np.array([[20,6,30, 27.8, 30.56, 13, 240]])
dt = pd.DataFrame(dt,columns =['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'])
prediction1 = RF.predict(dt)
prediction2 = XB.predict(dt)
print("RF Predicted Crop : ",prediction1)
print("XB Predicted Crop : ",prediction2)



"""## SAVE MODEL FOR USE

"""

import pickle

# Saving model to disk
pickle.dump(RF, open('model.pkl','wb'))

# Loading model to compare the results
load_model = pickle.load(open('model.pkl','rb'))
print(load_model.predict([[83, 45, 60, 28, 70.3, 7.0, 150.9]]))

